{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "from threading import Thread\n",
    "\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import process\n",
    "from intents import *\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from p_tqdm import p_umap\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix,\n",
    "                             precision_recall_fscore_support)\n",
    "from tqdm import tqdm\n",
    "from utils import print_confusion_matrix\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Prompt \n",
    "This could be used for one-vs-all classification. \n",
    "\n",
    "`intent_classification_one_feed_examples` is used for generating prompt where you supply both positive and negative examples to it and it will give you a prompt string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ = intent_classification_one_feed_examples(\n",
    "    intent_positive_examples=[\n",
    "        \"I want to buy shoes\",\n",
    "        \"I would like to buy a chocolate\",\n",
    "        \"How to purchase camera\",\n",
    "    ],\n",
    "    intent_negative_examples=[\n",
    "        \"I dance and sing\",\n",
    "        \"I want to drink water\",\n",
    "        \"Where can I talk to a real human here?\",\n",
    "    ],\n",
    "    intent=\"buy\",\n",
    ")\n",
    "classify_intents(in_, \"i want to buy\", return_entire_resp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ = intent_classification_many_feed_examples(\n",
    "    intents=[\"buy\", \"return\"],\n",
    "    intent_examples={\n",
    "        \"buy\": [\n",
    "            \"I want to buy shoes\",\n",
    "            \"I would like to buy a chocolate\",\n",
    "            \"How to purchase camera\",\n",
    "        ],\n",
    "        \"return\": [\"I want to return this product\", \"How does one exchange this?\"],\n",
    "    },\n",
    ")\n",
    "classify_intents(\n",
    "    in_, \"How do I return my shoes that I buy online yesterday?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at verloop specific intents\n",
    "\n",
    "Intents from verdan are saved in `data/verloop_intents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verloop_intents = json.load(open(\"../data/verloop_intents.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit classify_intents( intent_classification_many_feed_examples(intents=[\"AddItem\", \"Returns\", \"Refunds\"], intent_examples=verloop_intents),\"How do I add things on my list\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver_intents = {}\n",
    "for key, value in verloop_intents.items():\n",
    "    # Taking at most two intents considering prompt size constraints\n",
    "    ver_intents[key] = verloop_intents[key][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ = intent_classification_many_feed_examples(\n",
    "    intents= list(ver_intents.keys()), \n",
    "    intent_examples= ver_intents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"ada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intents_jsonl(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Because lambda functions can't be pickled, here insted of passing the prompt \n",
    "    as function parameters, it takes in_ and model as global prompt.\n",
    "    \"\"\"\n",
    "    for obj in jsonlines.open(f\"../data/{model}_{query_intent}.jsonl\"):\n",
    "        if query.MessageId == obj[\"MessageId\"]:\n",
    "            return\n",
    "    gpt3_class_input = in_\n",
    "    resp = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=f\"{gpt3_class_input}\\nS:{query.Message}\\n\",\n",
    "        max_tokens=10,\n",
    "        temperature=0,\n",
    "        logprobs=10,\n",
    "    )\n",
    "    o = resp.choices[0].text\n",
    "    write_output = {\n",
    "        \"time\": str(datetime.datetime.now()),\n",
    "        \"MessageId\": query.MessageId,\n",
    "        \"Message\": query.Message,\n",
    "        \"predicted\": o[o.find(\"I:\") + 3 : o.find(\"\\n\")],\n",
    "        \"logprobs\":json.dumps(resp.choices[0].logprobs),\n",
    "        \"text\" : o\n",
    "    }\n",
    "    open(f\"../data/{model}_{query_intent}.jsonl\", \"a\").write(json.dumps(write_output) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.read_csv(\"../data/predictions_with_groundtruth.csv\")\n",
    "df_preds = df_preds[df_preds[\"MessageId\"] != \"zoJ8RNwzaLxPTzW9x\"] # FALSE as a intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_iter = [row for _, row in df_preds.iterrows()]\n",
    "classify_intents_jsonl(rows_iter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(f\"../data/{model}.jsonl\", \"w\").write(\"\")\n",
    "with Pool(100) as p:\n",
    "    r = list(p.imap(classify_intents, rows_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "field = \"Refunds\"\n",
    "for obj in jsonlines.open(f\"../data/davinci_{field}.jsonl\"):\n",
    "    output.append(obj)\n",
    "df_out = pd.DataFrame(output)\n",
    "df_out = df_out[[\"MessageId\", \"predicted\"]].merge(\n",
    "    df_preds[[\"MessageId\", \"VerdanPrediction\", \"Ground Truth\"]], how=\"left\"\n",
    ")\n",
    "df_out[\"MadeUp\"] = df_out[\"predicted\"].apply(\n",
    "    lambda x: False if x in verloop_intents.keys() else True\n",
    ")\n",
    "df_out[\"CloseMatch\"] = df_out[\"predicted\"].apply(\n",
    "    lambda x: process.extract(x, list(verloop_intents.keys()), limit=1)[0][0]\n",
    ")\n",
    "\n",
    "df_out[\"predicted\"] = df_out[\"predicted\"].apply(lambda x : x if x==field else \"False\")\n",
    "clear_output()\n",
    "df_out = df_out[df_out[\"Ground Truth\"]==field]\n",
    "df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performace - GPT3 - Measured wrt Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(df_out))\n",
    "print(f'Accuracy : {accuracy_score(df_out[\"predicted\"], df_out[\"Ground Truth\"])}')\n",
    "print(f'PRF : {precision_recall_fscore_support(df_out[\"predicted\"], df_out[\"Ground Truth\"], average=\"macro\")}')\n",
    "cf_matrix = confusion_matrix(df_out[\"predicted\"], df_out[\"Ground Truth\"], labels=list(set(list(df_out[\"predicted\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performace - Verdan Predictions- Measured wrt Ground Truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_out))\n",
    "print(f'Accuracy : {accuracy_score(df_out[\"VerdanPrediction\"], df_out[\"Ground Truth\"])}')\n",
    "print(f'PRF : {precision_recall_fscore_support(df_out[\"VerdanPrediction\"], df_out[\"Ground Truth\"], average=\"macro\")}')\n",
    "cf_matrix = confusion_matrix(df_out[\"VerdanPrediction\"], df_out[\"Ground Truth\"], labels=list(set(list(df_out[\"VerdanPrediction\"]))))\n",
    "print_confusion_matrix(cf_matrix, list(set(list(df_out[\"VerdanPrediction\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performace - GPT3 Fuzzy Matched with Standard Intents- Measured wrt Ground Truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_out))\n",
    "print(f'Accuracy : {accuracy_score(df_out[\"CloseMatch\"], df_out[\"Ground Truth\"])}')\n",
    "print(f'PRF : {precision_recall_fscore_support(df_out[\"CloseMatch\"], df_out[\"Ground Truth\"], average=\"macro\")}')\n",
    "cf_matrix = confusion_matrix(df_out[\"CloseMatch\"], df_out[\"Ground Truth\"], labels=list(set(list(df_out[\"CloseMatch\"]))))\n",
    "print_confusion_matrix(cf_matrix, list(set(list(df_out[\"CloseMatch\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performace - GPT3 only predictions matching standard intents- Measured wrt Ground Truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_out = df_out[df_out.MadeUp==False]\n",
    "print(len(df_out))\n",
    "print(f'Accuracy : {accuracy_score(df_out[\"predicted\"], df_out[\"Ground Truth\"])}')\n",
    "print(f'PRF : {precision_recall_fscore_support(df_out[\"predicted\"], df_out[\"Ground Truth\"], average=\"macro\")}')\n",
    "cf_matrix = confusion_matrix(df_out[\"predicted\"], df_out[\"Ground Truth\"], labels=list(set(list(df_out[\"predicted\"]))))\n",
    "print_confusion_matrix(cf_matrix, list(set(list(df_out[\"predicted\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classificaition for each intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = {}\n",
    "\n",
    "for intent, examples in verloop_intents.items():\n",
    "    pos_examples = examples\n",
    "    neg_examples = [\n",
    "        examples if not (ver_int == intent) else []\n",
    "        for ver_int, examples in verloop_intents.items()\n",
    "    ]\n",
    "    neg_examples = [item for sublist in neg_examples for item in sublist]\n",
    "    intent_prompt[intent] = intent_classification_one_feed_examples(\n",
    "        intent_positive_examples=pos_examples,\n",
    "        intent_negative_examples=random.sample(neg_examples, 10),\n",
    "        intent=intent,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in intent_prompt.keys():\n",
    "    open(f'../data/davinci_{key}.jsonl', \"w\").write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_intent in intent_prompt.keys():\n",
    "    print(f\"Writing for {query_intent}\")\n",
    "    in_ =intent_prompt[query_intent]\n",
    "    with Pool(50) as p:\n",
    "        r = list(p.imap(classify_intents_jsonl, rows_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
