{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymagnitude import Magnitude\n",
    "vectors = Magnitude(\"glove/heavy/glove.twitter.27B.200d.magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flyingcat'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectors.most_similar_to_given(\"nirant\", [\"flyingcat\", \"dog\", \"unicorn\",\"deadinside\", \"indian\"])\n",
    "vectors.most_similar_to_given(\"nirant\", [\"flyingcat\", \"dog\", \"unicorn\",\"deadinside\", \"indian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WritingsOfAbrahamLincoln.txt\", \"r\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_sample = content[:999999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 1.58 s, total: 11.7 s\n",
      "Wall time: 7.84 s\n"
     ]
    }
   ],
   "source": [
    "%time doc = nlp(content_sample, disable=[\"textcat\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78404, 8806)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = list(map(str, set(tokens)))\n",
    "# search_space = unique_tokens.remove(word); search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_zones = [\n",
    "    [\"gettysburg\", \"commencement\", \"generality\"],\n",
    "    [\"wife\", \"husband\", \"daughter\", \"father\"],\n",
    "#     [\"slavery\", \"antislavery\", \"slave\", \"slavedealer\", \"dealer\"],\n",
    "    [\"war\", \"battle\", \"depression\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gettysburg\n",
      "Removing commencement\n",
      "Removing generality\n",
      "gettysburg commencement generality resolution,â€”that\n",
      "Removing wife\n",
      "Removing husband\n",
      "Removing daughter\n",
      "Removing father\n",
      "wife husband daughter father girl\n",
      "Removing war\n",
      "Removing battle\n",
      "Removing depression\n",
      "war battle depression cession\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "for wordlist in seed_zones:\n",
    "#     for word in wordlist:\n",
    "    word = \" \".join(wordlist)\n",
    "    search_space = deepcopy(unique_tokens)\n",
    "    try:\n",
    "        for wordx in wordlist:\n",
    "            print(f\"Removing {wordx}\")\n",
    "            search_space.remove(wordx)\n",
    "    except:\n",
    "        search_space = unique_tokens\n",
    "    print(word, vectors.most_similar_to_given(word, search_space))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
