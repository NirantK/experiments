{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath: Path = \"Movie_Reviews.txt\"):\n",
    "    with Path(filepath).open(\"r\") as f:\n",
    "        movie_review_text:str = f.read()\n",
    "        return movie_review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264133428, 132066677)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_text = read_file(filepath=\"Movie_Reviews.txt\")\n",
    "movie_review_text.__sizeof__(), len(movie_review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm # in case you forgot to download this earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Official spaCy docs\n",
    "https://spacy.io/usage/spacy-101#annotations-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting to our Data\n",
    "\n",
    "### Disable Unused Components \n",
    "spaCy allows us to disable components of the `pipeline` which we don't use. Since we will be using this primarily for tokenization, we disable everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\", \"parser\", \"textcat\"])\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase Max Length\n",
    "By default, spaCy allows you to parse only text string of 100,000 characters at a time. This is a safeguard to prevent memory overflow issues. \n",
    "\n",
    "From spaCy's estimates, they need about 1G of memory for every 100K characters -- if using all the entire pipeline. Since, we're not using the entire pipeline -- I'm increasing the `max_length` to account for our input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132066678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.max_length = len(movie_review_text)+1\n",
    "nlp.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 5.42 s, total: 2min 5s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%timeit -n 3\n",
    "doc = nlp(movie_review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Bag of Tokens (Words)\n",
    "In popular classical ML pipelines, we will need a bag of tokens (words) for classification. Extracting that from the NLP library is often 1 extra step. We profile that as well, to get a better sense of how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 s, sys: 1.1 s, total: 19.8 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sense of Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_cntr = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens: 261647\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique Tokens: {len(token_cntr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At almost ~270K, this is a large vocabulary and can lead to too much sparsity in our matrix computations. Let's try to reduce the vocabulary size while still retaining the maximum signal we can. \n",
    "\n",
    "Using popular convention, we try these next:\n",
    "1. Keep tokens with minimum frequency = 3\n",
    "2. Lowercase all tokens and then set minimum frequency = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing our Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cntr = {k: v for k, v in token_cntr.items() if v >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping all rare tokens, min_freq = 2, we have:\n",
      "Unique Tokens: 97469\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"After dropping all rare tokens, min_freq = {min_freq}, we have:\\nUnique Tokens: {len(token_cntr)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still larger than what I'd like. Let's see if we can get a small vocabulary with lowercase tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.5 s, sys: 2.86 s, total: 31.4 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%time lowercase_token_cntr = Counter([token.text.lower() for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens: 217491\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique Tokens: {len(lowercase_token_cntr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_token_cntr = {k: v for k, v in lowercase_token_cntr.items() if v >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping all rare tokens, min_freq = 3, we have:\n",
      "Unique Tokens: 81408\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"After dropping all rare tokens, min_freq = {min_freq}, we have:\\nUnique Tokens: {len(lowercase_token_cntr)}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
