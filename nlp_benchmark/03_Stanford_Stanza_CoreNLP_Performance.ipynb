{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath: Path = \"Movie_Reviews.txt\"):\n",
    "    with Path(filepath).open(\"r\") as f:\n",
    "        movie_review_text:str = f.read()\n",
    "        return movie_review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264133428, 132066677)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_text = read_file(filepath=\"Movie_Reviews.txt\")\n",
    "movie_review_text.__sizeof__(), len(movie_review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the Official [Docs](https://stanfordnlp.github.io/stanza/installation_usage.html#getting-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 5.18MB/s]                    \n",
      "2020-05-21 14:46:09 INFO: Downloading default packages for language: en (English)...\n",
      "2020-05-21 14:46:10 INFO: File exists: /Users/nirant/stanza_resources/en/default.zip.\n",
      "2020-05-21 14:46:14 INFO: Finished downloading models and saved to /Users/nirant/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')       # This downloads the English models for the neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-21 14:46:14 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-05-21 14:46:14 INFO: Use device: cpu\n",
      "2020-05-21 14:46:14 INFO: Loading: tokenize\n",
      "2020-05-21 14:46:14 INFO: Loading: pos\n",
      "2020-05-21 14:46:15 INFO: Loading: lemma\n",
      "2020-05-21 14:46:15 INFO: Loading: depparse\n",
      "2020-05-21 14:46:16 INFO: Loading: ner\n",
      "2020-05-21 14:46:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', verbose=True) # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenize': <stanza.pipeline.tokenize_processor.TokenizeProcessor at 0x14c04df10>,\n",
       " 'pos': <stanza.pipeline.pos_processor.POSProcessor at 0x14c05dc10>,\n",
       " 'lemma': <stanza.pipeline.lemma_processor.LemmaProcessor at 0x14d53d490>,\n",
       " 'depparse': <stanza.pipeline.depparse_processor.DepparseProcessor at 0x14d2cef90>,\n",
       " 'ner': <stanza.pipeline.ner_processor.NERProcessor at 0x1220da190>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanza uses a nested data format (doc -> sentences -> words/tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barack',\n",
       " 'Obama',\n",
       " 'was',\n",
       " 'born',\n",
       " 'in',\n",
       " 'Hawaii',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'elected',\n",
       " 'president',\n",
       " 'in',\n",
       " '2008',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        words.append(word.text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', '.', 'He', 'was', 'elected', 'president', 'in', '2008', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for sentence in doc.sentences:\n",
    "    for token in sentence.tokens:\n",
    "        tokens.append(token.text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting to our Data\n",
    "\n",
    "### Disable Unused Components \n",
    "Stanza allows us to select processors in the `Pipeline` which we want to use. Since we will be using this primarily for tokenization, we skip everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(\"en\", processors=\"tokenize\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenize': <stanza.pipeline.tokenize_processor.TokenizeProcessor at 0x11a05b950>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 8.72 s, total: 3min 9s\n",
      "Wall time: 55.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%timeit -n 3\n",
    "doc = nlp(movie_review_text[:100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Bag of Tokens (Words)\n",
    "In popular classical ML pipelines, we will need a bag of tokens (words) for classification. Extracting that from the NLP library is often 1 extra step. We profile that as well, to get a better sense of how long it takes.\n",
    "\n",
    "Since we need tokens, not words, we make a small change from the Example earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 ms, sys: 809 Âµs, total: 13.6 ms\n",
      "Wall time: 13.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens = []\n",
    "for sentence in doc.sentences:\n",
    "    for token in sentence.tokens:\n",
    "        tokens.append(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sense of Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_cntr = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens: 4519\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique Tokens: {len(token_cntr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At almost ~270K, this is a large vocabulary and can lead to too much sparsity in our matrix computations. Let's try to reduce the vocabulary size while still retaining the maximum signal we can. \n",
    "\n",
    "Using popular convention, we try these next:\n",
    "1. Keep tokens with minimum frequency = 3\n",
    "2. Lowercase all tokens and then set minimum frequency = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing our Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cntr = {k: v for k, v in token_cntr.items() if v >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"After dropping all rare tokens, min_freq = {min_freq}, we have:\\nUnique Tokens: {len(token_cntr)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still larger than what I'd like. Let's see if we can get a small vocabulary with lowercase tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lowercase_token_cntr = Counter([token.text.lower() for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique Tokens: {len(lowercase_token_cntr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_token_cntr = {k: v for k, v in lowercase_token_cntr.items() if v >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"After dropping all rare tokens, min_freq = {min_freq}, we have:\\nUnique Tokens: {len(lowercase_token_cntr)}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
