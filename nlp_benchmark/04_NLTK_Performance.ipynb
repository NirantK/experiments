{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath: Path = \"Movie_Reviews.txt\"):\n",
    "    with Path(filepath).open(\"r\") as f:\n",
    "        movie_review_text:str = f.read()\n",
    "        return movie_review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264133428, 132066677)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_text = read_file(filepath=\"Movie_Reviews.txt\")\n",
    "movie_review_text.__sizeof__(), len(movie_review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm # in case you forgot to download this earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Official NLTK docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'How', 'does', 'it', 'deal', 'with', 'this', 'parenthesis', '?', ')', '``', 'It', 'should', 'be', 'part', 'of', 'the', 'previous', 'sentence', '.', \"''\", '``', '(', 'And', 'the', 'same', 'with', 'this', 'one', '.', ')', \"''\", '(', \"'And\", 'this', 'one', '!', \"'\", ')', '``', '(', \"'\", '(', 'And', '(', 'this', ')', ')', \"'\", '?', ')', \"''\", '[', '(', 'and', 'this', '.', ')', ']']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = \"\"\"\n",
    "(How does it deal with this parenthesis?)  \"It should be part of the\n",
    "previous sentence.\" \"(And the same with this one.)\" ('And this one!')\n",
    "\"('(And (this)) '?)\" [(and this. )]\n",
    "\"\"\"\n",
    "# print(sent_tokenize(text))\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting to our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 3.87 s, total: 2min 56s\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %%timeit -n 3\n",
    "tokens = word_tokenize(movie_review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sense of Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_cntr = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens: 301240\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique Tokens: {len(token_cntr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At ~300K, this is a large vocabulary and can lead to too much sparsity in our matrix computations. Let's try to reduce the vocabulary size while still retaining the maximum signal we can. \n",
    "\n",
    "Using popular convention, we try these next:\n",
    "1. Keep tokens with minimum frequency = 3\n",
    "2. Lowercase all tokens and then set minimum frequency = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing our Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cntr = {k: v for k, v in token_cntr.items() if v >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping all rare tokens, min_freq = 3, we have:\n",
      "Unique Tokens: 102458\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"After dropping all rare tokens, min_freq = {min_freq}, we have:\\nUnique Tokens: {len(token_cntr)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still larger than what I'd like. Let's see if we can get a small vocabulary with lowercase tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.72 s, sys: 698 ms, total: 8.42 s\n",
      "Wall time: 8.43 s\n"
     ]
    }
   ],
   "source": [
    "%time lowercase_token_cntr = Counter([token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tokens: 254678\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique Tokens: {len(lowercase_token_cntr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_token_cntr = {k: v for k, v in lowercase_token_cntr.items() if v >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping all rare tokens, min_freq = 3, we have:\n",
      "Unique Tokens: 86359\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"After dropping all rare tokens, min_freq = {min_freq}, we have:\\nUnique Tokens: {len(lowercase_token_cntr)}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
